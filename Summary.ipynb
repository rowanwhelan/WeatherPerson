{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "8d6f0c56-a331-44cb-a866-0a8a64a2eddc",
      "cell_type": "markdown",
      "source": "TASK:\nI was tasked with creating a model which could predict the maximum temperature on a given day. To do this I started by attempting to implement a linear regression model to try to predict the maximum temperature on a given day. However this was relatively inaffective. I used this code, which calls a linear regression class.",
      "metadata": {}
    },
    {
      "id": "54fcd2db-0a1b-4e66-b2bd-77465981dc22",
      "cell_type": "markdown",
      "source": "IMPORTANT LINK: https://github.com/rowanwhelan/WeatherPerson",
      "metadata": {}
    },
    {
      "id": "0af54dd9-4ba5-4990-a025-e2438f625ea7",
      "cell_type": "code",
      "source": "import csv\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport requests\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom keras.models import model_from_json\nfrom meteostat import Point, Daily\nfrom keys.OpenWeather import key\nimport pandas as pd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "59b5dbd4-026d-4f89-a27b-b2fe194edb5e",
      "cell_type": "code",
      "source": "import math\nimport random\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\n\n#/\ndef gradientDescent(Theta0, learnrate, function, partial, epsilon):\n    theta = Theta0\n    t = 0 \n    while True:\n        t = t + 1\n        thetaTemp = theta\n        theta = theta - learnrate(partial(theta))\n        if math.abs(function(theta) - function(thetaTemp)) < epsilon:\n            break\n    return theta\n#/\n\nclass Regression:\n    def __init__(self, iterations: int = 100, learning_rate: float = 1, dimension: int = 1):\n        self.iterations = iterations\n        self.alpha = learning_rate\n\n        self.intercept = 0\n        self.weight_vector = [0] * dimension\n\n    def fit(self, x, y):    \n        #initialize parameters \n        intercept = 0\n        weight_vector = [ random.random() for col in range(len(x[0])) ]\n\n\n        #begin fitting data\n        iteration_number = 0 \n        while iteration_number < self.iterations:\n            iteration_number += 1\n            intercept_gradient, weight_vector_gradient = self.compute_gradient(x, y, intercept, weight_vector)\n            intercept, weight_vector = self.update(intercept, intercept_gradient, weight_vector, weight_vector_gradient)\n        return intercept, weight_vector\n        \n\n    def compute_gradient(self, x,y,intercept, weight_vector):\n        intercept_gradient, weight_vector_gradient = 0, [0] * len(x[0])\n        for i in range(len(x)):\n            #predict with your model then see how accurate your prediction is\n            y_i_hat = intercept + sum((x[i][j] * weight_vector[j]) for j in range(len(x[0])))\n            partial_error = 2 * (y[i] - y_i_hat)\n\n            for j in range(len(x[0])):\n                #update your values based on your predictions \n                weight_vector_gradient[j] += partial_error * (x[i][j]) / len(x)\n                intercept_gradient += partial_error/ len(x)\n        return intercept_gradient, weight_vector_gradient\n    \n    def update(self, intercept, intercept_gradient, weight_vector, weight_vector_gradient):\n        intercept += intercept_gradient * self.alpha\n        for j in range(len(weight_vector)):\n            weight_vector[j] += weight_vector_gradient[j] * self.alpha\n        return intercept, weight_vector\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1cada955-4ee3-4a91-86f2-bf9f7e35bc56",
      "cell_type": "code",
      "source": "from Regression import Regression\n\n\nreg = Regression(1000000,.0001,1)\n\nbelvedere_castle_tempartures = [61,41,47,55,67,58,49,52,54]\nmidway_international_airport_temperatures = [53,45,46,61,73,72,44,48,49]\nbergstrom_temperatures = [70,49,69,82,79,84,90,83,75]\nmiami_temperatures = [79,81,79,79,82,84,82,80,86]\n\nz = [ [data] for data in range(1,10)]\n\nbelv_int, belv_values = reg.fit(z,belvedere_castle_tempartures)\nbelv_prediction = belv_int + belv_values[0]*6\nprint(\"the model predicts:\", belv_prediction, \"for belvedere\")\n\nreg2 = Regression(1000000,0.0001,1)\nmid_int, mid_values = reg2.fit(z,midway_international_airport_temperatures)\nmid_predicition = mid_int + mid_values[0]*6\nprint(\"the model predicts:\", mid_predicition, \"for midway\")\n\nreg3 = Regression(1000000,0.0001,1)\nberg_int, berg_values = reg3.fit(z,bergstrom_temperatures)\nberg_prediction = berg_int + berg_values[0]*6\nprint(\"the model predicts:\", berg_prediction, \"for bergstrom\")\n\nreg4 = Regression(1000000,0.0001,1)\nmiami_int, miama_values = reg4.fit(z,miami_temperatures)\nmiami_prediction = miami_int + miama_values[0]*6\nprint(\"the model predicts:\", miami_prediction, \"for miami\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "19d37d88-05a2-4086-a008-16e1837a6463",
      "cell_type": "markdown",
      "source": "However, the weather too complicated to be modeled by a simple linear regression model. This is reflected in the accuracy of this model.\nBelvedere: 1/4 days correct        Average Loss: 4.25 degrees\nMidway: 1/4 days correct           Average Loss: 11.75 degrees\nBergstrom: 0/4 days correct        Average Loss: 3.5 degrees\nMiami: 1/4 days correct            Average Loss: 2.5 degrees\nClearly this would not be good enough to gamble on. So I decided to instead try to capture the changing hot and cold fronts with different meteorological data.",
      "metadata": {}
    },
    {
      "id": "cd5cd85a-169c-46b2-a724-187e366cac28",
      "cell_type": "markdown",
      "source": "I decided to capture these changing phenomenon with the following features ( Previous Days Max Temperature, Precipitation, Wind Direction, Wind Speed, Air Pressure, Precipitation factoring in snow, Sunlight, Humidity, Average co2 in the air over a 24 hour span, and the change in co2 over 24 hours )\n\nTo do this I had to get this data from online sources. I used these methods.",
      "metadata": {}
    },
    {
      "id": "820e16ae-ad33-4816-858e-43913e5a1ac3",
      "cell_type": "code",
      "source": "from datetime import datetime\nimport math\nimport time\nimport numpy as np\nimport requests,sys \nsys.path.append(\"../\")\nfrom CS_546.keys.NCEI import key\n\ndef write_data(data, path):\n    if len(data) == 0:\n        print(\"Error\")\n        return\n    try:\n        # Save the data to the CSV file\n        np.savetxt(path, data, delimiter=\",\")\n        print(f\"Data has been successfully written to '{path}'.\")\n    except Exception as e:\n        print(\"An error occurred while writing to the CSV file:\", e)\n\ndef index_of_day(date_str):\n    date_str = date_str[0:10]\n    try:\n        # Parse the date string into a datetime object\n        date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n        # Get the day of the year\n        day_of_year = date_obj.timetuple().tm_yday\n        year = int(date_str[0:4])\n        day_of_year = day_of_year + ((-2000 + year)*365 + math.floor((-2000 + year) / 4))\n        return day_of_year\n    except ValueError:\n        # Handle invalid date string\n        print(\"Invalid date format. Please provide date in YYYY-MM-DD format.\")\n\ndef tabulate_response(response,table):\n    if response.status_code == 200:\n        data = response.json()\n        if data == {}:\n            return table\n        for observation in data['results']:\n            date = observation['date']    \n            value = observation['value']\n            print(value, index_of_day(date)-1)\n            table[index_of_day(date)-1][1] = value\n    else:\n        print(f'Error: {response.status_code} - {response.text}')\n    return table\n\ndef collect_NCEI_data(station,data_set,path):\n    retry_delay = 1\n    table = np.array([[1,0]])\n    for num in range (1,8765):\n        table = np.vstack((table,[[num+1,0]]))\n        \n    for i in range(2000, 2023):\n        time.sleep(retry_delay)\n        start_date = str(i)+'-01-01'\n        end_date = str(i)+'-12-31'\n        api_url = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/data'\n        params = {\n            'datasetid': data_set,\n            'startdate': start_date,\n            'enddate': end_date,\n            'stationid': station,\n            'format': 'json',\n            'limit': '1000',\n            'datatypeid': ['PRCP']\n        }\n        headers = {'token': key}\n        response = requests.get(api_url, params=params, headers=headers)\n        table = tabulate_response(response, table)\n    write_data(table,path)\n    return table\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ac823c8a-14f8-4e71-8328-b9febdc4a93c",
      "cell_type": "code",
      "source": "import requests,sys \nsys.path.append(\"../\")\n\ndef collect_NWS_data():\n    #Belvedere\n    stationid = 'KW035'\n    start = '1980-01-01T00:00:00+19:00'\n    end = '2023-12-21T24:00:00+19:00'\n    api_url = f'https://api.weather.gov/stations/{start, end, stationid}/observations'\n    response = requests.get(api_url, headers= {'User-Agent': 'WeatherPredictorApp/1.0 contact rwhelan340@gmail.com', 'Accept': 'application/json'})\n\n    \n    # Check if request was successful\n    if response.status_code == 200:\n    # Parse the JSON response\n        data = response.json()\n    # Print the entire response for inspection\n        print(data)\n    else:\n        print('Error:', response.status_code)\n        print('Response Content:', response.text)  # Print response content for inspection\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "822ca099-5e80-4745-9210-f682738d3cf4",
      "cell_type": "code",
      "source": "from datetime import datetime\nimport math\nimport sys\nimport time\nimport numpy as np\nimport pyowm\nfrom pyowm.utils.config import get_config_from\nsys.path.append(\"../\")\nfrom CS_546.keys.OpenWeather import key\n\ndef get_co_in_air(latitude, longitude, start_date, end_date):\n    config_dict = pyowm.utils.config.get_default_config()\n    owm = pyowm.OWM(key,config_dict)\n    mgr = owm.airpollution_manager()\n    co2_vals = np.array([])\n    retry_count = 5\n    retry_delay = 2 # seconds\n\n    for _ in range(retry_count):\n        try:\n            list_of_historical_values = mgr.air_quality_history_at_coords(latitude, longitude, start=start_date, end=end_date)\n            # Each item in the list_of_historical_values is an Air  Status object \n            for air_status in list_of_historical_values:\n                co2_vals = np.append(co2_vals,air_status.co)\n            break\n\n        except pyowm.commons.exceptions.NotFoundError:\n            print(\"Historical weather data not found for the specified location and date.\")\n            break\n        except Exception as e:\n            print(\"An error occurred:\", e)\n            print(\"Retrying...\")\n            time.sleep(retry_delay)\n    else:\n        print(\"Failed to retrieve historical weather data after multiple retries.\")\n    \n    return co2_vals\n\ndef write_data(data, path):\n    if len(data) == 0:\n        print(\"Error\")\n        return\n    try:\n        # Save the data to the CSV file\n        np.savetxt(path, data, delimiter=\",\")\n        print(f\"Data has been successfully written to '{path}'.\")\n    except Exception as e:\n        print(\"An error occurred while writing to the CSV file:\", e)\n        \ndef collect_co2_data(latitude, longitude, path):\n    complete_co2_table = np.array([])\n    start = datetime(2000,1,1)\n    end = datetime(2023,12,31)\n    complete_co2_table = np.append(complete_co2_table, get_co_in_air(latitude, longitude,start,end_date = end))\n    print(len(complete_co2_table))\n    #write_data(complete_co2_table, path)\n    return\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "661e52cd-0956-47c6-b7bb-ae0124dac646",
      "cell_type": "code",
      "source": "from datetime import datetime\nfrom meteostat import Point, Daily\nimport numpy as np\n\ndef collect_air_stats(latitude, longitude, path):\n    location = Point(latitude,longitude)\n    data = Daily(location, start = datetime(2000,1,1,0,0), end= datetime(2023,12,31,0,0))\n    data = data.fetch()\n    data = data[['tmax', 'prcp', 'wdir', 'wspd', 'pres']]\n    data = data.fillna(-1)\n            \n    np.savetxt(path, data, delimiter=',')\n    print('array successfully saved')\n    \n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4d8dfc1a-3122-4a95-a1cb-f6b6278ed015",
      "cell_type": "code",
      "source": "import math\nfrom matplotlib import pyplot as plt\nimport numpy as np, datetime, ephem\n\n#function to convert from year day to the ephem internal values\ndef date_to_ephem_date(year,day):\n    return ephem.Date(36525.0 + ((-2000 + year) *365) + day + math.floor((-2000 + year) / 4) -0.5)\n\ndef collect_sunlight_hours(latitude, longitude, print_val, path):\n    observer = ephem.Observer()\n    observer.lat, observer.lon = latitude, longitude\n    all_dates = np.array([])\n    all_daylight_hours = np.array([])\n    all_dates, all_daylight_hours = create_sunlight_table(observer, all_dates, all_daylight_hours)\n    if print_val:\n        plot_sunlight_table(all_dates, all_daylight_hours)\n    zipped_array = np.vstack((all_dates, all_daylight_hours)).ravel(order='F')\n    np.savetxt(path, all_daylight_hours, delimiter=',')\n    print('array successfully saved')\n\ndef create_sunlight_table(observer, all_dates, all_daylight_hours):\n    for year in range(2000,2024):\n        dates = np.array([])\n        daylight_hours = np.array([])\n        days = 366\n        if(year%4 == 0):\n            days = days+1\n        for day in range(1,days):\n            date = f'{year}-{day:03d}'\n            observer.date = datetime.datetime.strptime(date, '%Y-%j')\n            sunrise = observer.next_rising(ephem.Sun())\n            sunset = observer.next_setting(ephem.Sun())\n            #The sunset on the day happens in the morning before the sunrise, so we must update the day to compensate for this\n            if sunset - sunrise < 0:\n                #There is a fringe case where if the sunset happens very close to the changing of days, so this if statement catches that case and handles it by removing 24 hours from that calculation\n                if sunset - date_to_ephem_date(year, day) < 0.007:\n                    sunset = sunset + 1\n                #Otherwise the algorithm continues as normal finding the next sunset\n                else:\n                    date = f'{year}-{day+1:03d}'\n                    if day == days-1:\n                        date = f'{year+1}-{1:03d}'\n                    observer.date = datetime.datetime.strptime(date, '%Y-%j')\n                    sunset = observer.next_setting(ephem.Sun())\n            #Now that edge cases have been accounted for the algorithm computes the amount of sunlight and appends it to the return array   \n            daylight_duration = sunset - sunrise\n            daylight_hours = np.append(daylight_hours, daylight_duration * 24)\n            dates = np.append(dates,int(date_to_ephem_date(year,day)))  \n        all_dates = np.append(all_dates,dates)\n        all_daylight_hours = np.append(all_daylight_hours,daylight_hours)\n    return all_dates, all_daylight_hours\n\ndef plot_sunlight_table(all_dates, all_daylight_hours):\n    plt.figure(figsize=(15, 8))\n    plt.plot(all_dates, all_daylight_hours, label='Daylight')\n    plt.title('Daylight Hours')\n    plt.xlabel('Date')\n    plt.ylabel('Daylight Hours')\n    plt.show()\n    \n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "59fc2389-83ae-44d7-84fa-4bfb5f3a4c8a",
      "cell_type": "markdown",
      "source": "It is important to mention that Austin, Texas, doesn't record CO2 data, so every method will have two versions to accommodate the different data. I will analyze whether this affects accuracy later.",
      "metadata": {}
    },
    {
      "id": "ea78e286-5439-4a1d-ba4c-99fb1dd484ff",
      "cell_type": "markdown",
      "source": "The overall structure I used to train my models follows the pattern of compiling the table, formatting it into training and testing data, and then training and testing the model.",
      "metadata": {}
    },
    {
      "id": "0c50ceb4-83ad-40da-a778-21cab9f9910a",
      "cell_type": "code",
      "source": "def complete_NN(name, prev):\n    print(f'Training a model for {name}')\n    data = compile_tables(name, prev)\n    testing_labels, testing_data, data_labels, data = format_data(data)\n    model = NN(data.astype(float), data_labels.astype(float))\n    test_model(model,testing_data.astype(float),testing_labels.astype(float))\n    return model",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ac18b9da-1143-4c6e-9a45-66bfbf93166c",
      "cell_type": "code",
      "source": "def complete_Berg_NN(name, prev):\n    print(f'Training a model for {name}')\n    data = compile_tables(name, prev)\n    testing_labels, testing_data, data_labels, data = format_data(data)\n    model = Berg_NN(data.astype(float), data_labels.astype(float))\n    test_model(model,testing_data.astype(float),testing_labels.astype(float))\n    return model",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f0f1a69e-a040-423e-a9e9-fde6665f7c8e",
      "cell_type": "markdown",
      "source": "Now that the general layout of the solution is clear I will explain each of the four methods. The first of these methods opens the csv files the data is saved as an compiles the data into one table.",
      "metadata": {}
    },
    {
      "id": "31730500-4625-4a16-b5e6-e1d911d6167f",
      "cell_type": "code",
      "source": "def compile_tables(name,prev_temp):\n    filename = f'C:/Users/rwhel/OneDrive/Desktop/Notes/College/CS_546/data/{name}_Air_Pressure.csv'\n    meteo_array = read_data(filename)\n    filename = f'C:/Users/rwhel/OneDrive/Desktop/Notes/College/CS_546/data/{name}_NCEI.csv'\n    ncei_array = read_data(filename)\n    filename = f'C:/Users/rwhel/OneDrive/Desktop/Notes/College/CS_546/data/{name}_Sunlight.csv'\n    sun_array = np.genfromtxt(filename, delimiter=',')\n    # sun array data is taken farther back, but I have decided to only use the last 23 years of data to match the other sources\n    original_shape = (2, len(sun_array) // 2)\n    unzipped_array = sun_array.reshape(original_shape, order='F')\n    sun_array = unzipped_array[1]\n    padded_sun_array = np.pad(sun_array, (8765-8401, 0), 'constant', constant_values=0)\n    filename = f'C:/Users/rwhel/OneDrive/Desktop/Notes/College/CS_546/data/{name}_humidity.csv'\n    #humidity data only goes back to 2009 so you need to add \n    humid_array = read_data(filename)\n    humid_array = humid_array[:,1]\n    padded_humid_array = np.pad(humid_array, ((3290), (0)), mode='constant', constant_values=0)\n    \n    temps = meteo_array[:,0]\n    hist_temp = create_previous_temparatures(temps, prev_temp)\n\n    \n    if name != 'Bergstrom':\n        filename = f'C:/Users/rwhel/OneDrive/Desktop/Notes/College/CS_546/data/{name}_Air_c02.csv'\n        co2_table = read_data(filename)\n        co2_table = co2_table[-26295:]\n        co2_table = co2_reshape(co2_table)\n        combined_array = np.column_stack((meteo_array[:, 0], celsius_to_fahrenheit(hist_temp[:,0]), celsius_to_fahrenheit(hist_temp[:,1]), celsius_to_fahrenheit(hist_temp[:,2]), meteo_array[:, 1], meteo_array[:, 2], meteo_array[:, 3], meteo_array[:, 4], ncei_array[:, 1], padded_sun_array, padded_humid_array, co2_table[:, 0], co2_table[:, 1]))\n        return combined_array\n    else:\n        combined_array = np.column_stack((meteo_array[:, 0], celsius_to_fahrenheit(hist_temp[:,0]), celsius_to_fahrenheit(hist_temp[:,1]), celsius_to_fahrenheit(hist_temp[:,2]), meteo_array[:, 1], meteo_array[:, 2], meteo_array[:, 3], meteo_array[:, 4], ncei_array[:, 1], padded_sun_array, padded_humid_array))\n        return combined_array\n# pass in a n x 1 np array and three manually inputted temperatures get back a n x 3 np array\ndef create_previous_temparatures(data, temps):\n    second = np.array([temps[1], temps[2], data[0]])\n    third = np.array([temps[2], data[0],data[1]])\n    final = np.row_stack([temps,second,third])\n    for i in range(3,len(data)):\n        slice = np.array([data[i-3], data[i-2], data[i-1]])\n        final = np.row_stack([final, slice])\n    return final\n# CO2 data was taken every 8 hours, so I decided to have the algorithm learn based on the slope of the day and the average co2        \ndef co2_reshape(table):\n    # Reshape the original table to have three columns\n    reshaped_table = table.reshape(-1, 3)\n\n    # Calculate the average of every three points\n    averages = np.mean(reshaped_table, axis=1)\n\n    # Calculate the slope between each pair of consecutive points\n    slopes = np.diff(reshaped_table, axis=1)[:, 1] / 2  # Calculate the average of the differences as the slope\n\n    # Stack the averages and slopes to create the final 8765 x 2 table\n    final_table = np.column_stack((averages, slopes))\n    return final_table",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3e8a61c4-6471-4ae9-a65e-b0337a2a4a61",
      "cell_type": "markdown",
      "source": "Now that the method to get the tables is defined, the data has to be divided into testing and training sets. This is done by arbitrarily selected the last 100 days to be testing data.",
      "metadata": {}
    },
    {
      "id": "c51266ed-0cb8-46e2-a8d4-172475364b29",
      "cell_type": "code",
      "source": "def format_data(data):\n    labels = (data[:,0].astype(float) * 1.8) + 32\n    testing_labels = labels[-100:]\n    labels = labels[0:len(labels)-100]\n    data = data[:,1::]\n    testing_data = data[-100:]\n    data = data[0:len(data)-100]\n    return testing_labels, testing_data, labels, data",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "46ad1de1-aab3-4ff1-9fe3-cf77f4c1f441",
      "cell_type": "markdown",
      "source": "Now the data is fully formated and we can create our model.",
      "metadata": {}
    },
    {
      "id": "aacbaf3e-0827-4117-bc4c-32a41d528622",
      "cell_type": "code",
      "source": "class ClimateNN(nn.Module):\n        def __init__(self):\n            super(ClimateNN, self).__init__()\n            self.fc1 = nn.Linear(12, 128)  # Input layer\n            self.fc2 = nn.Linear(128,64)\n            self.fc3 = nn.Linear(64, 32)   # Hidden layer\n            self.fc4 = nn.Linear(32, 1)    # Output layer\n\n        def forward(self, x):\n            x = torch.relu(self.fc1(x))\n            x = torch.relu(self.fc2(x))\n            x = torch.relu(self.fc3(x))\n            x = self.fc4(x)\n            return x\n        \n        def predict(self, input_data):\n            input_tensor = torch.tensor(input_data, dtype=torch.float32)\n            with torch.no_grad():\n                self.eval()  # Set model to evaluation mode\n                output_tensor = self(input_tensor)\n                # If you have a single output neuron (e.g., regression), you might do:\n                predicted_output = output_tensor.item()\n                return predicted_output\n\ndef NN(data,labels):\n    # Convert numpy arrays to PyTorch tensors\n    X = torch.tensor(data, dtype=torch.float32)\n    y = torch.tensor(labels, dtype=torch.float32)\n\n\n    # Initialize the model, loss function, and optimizer\n    model = ClimateNN()\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n\n    # Training the model\n    epochs = 400\n    batch_size = 32\n    for epoch in range(epochs):\n        for i in range(0, len(X), batch_size):\n            inputs = X[i:i+batch_size]\n            batch_labels = y[i:i+batch_size].view(-1,1)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, batch_labels)\n            loss.backward()\n            optimizer.step()\n\n        if epoch % 100 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n    print('Finished Training')\n    return model",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d5c761fb-972a-4bae-9c81-cf9b80aef596",
      "cell_type": "code",
      "source": "class BergClimateNN(nn.Module):\n        def __init__(self):\n            super(BergClimateNN, self).__init__()\n            self.fc1 = nn.Linear(10, 128)  # Input layer\n            self.fc2 = nn.Linear(128,64)\n            self.fc3 = nn.Linear(64, 32)   # Hidden layer\n            self.fc4 = nn.Linear(32, 1)    # Output layer\n\n        def forward(self, x):\n            x = torch.relu(self.fc1(x))\n            x = torch.relu(self.fc2(x))\n            x = torch.relu(self.fc3(x))\n            x = self.fc4(x)\n            return x\n        \n        def predict(self, input_data):\n            input_tensor = torch.tensor(input_data, dtype=torch.float32)\n            with torch.no_grad():\n                self.eval()  # Set model to evaluation mode\n                output_tensor = self(input_tensor)\n                # If you have a single output neuron (e.g., regression), you might do:\n                predicted_output = output_tensor.item()\n                return predicted_output\ndef Berg_NN(data,labels):\n    # Convert numpy arrays to PyTorch tensors\n    X = torch.tensor(data, dtype=torch.float32)\n    y = torch.tensor(labels, dtype=torch.float32)\n\n\n    # Initialize the model, loss function, and optimizer\n    model = BergClimateNN()\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n\n    # Training the model\n    epochs = 400\n    batch_size = 32\n    for epoch in range(epochs):\n        for i in range(0, len(X), batch_size):\n            inputs = X[i:i+batch_size]\n            batch_labels = y[i:i+batch_size].view(-1,1)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, batch_labels)\n            loss.backward()\n            optimizer.step()\n\n        if epoch % 100 == 0:\n            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n    print('Finished Training')\n    return model",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "79498a25-502e-4e68-b289-654148033af1",
      "cell_type": "markdown",
      "source": "The NN and BergNN methods are called to instantiate ClimateNN and BergClimateNN objects, train, and return them. This part of the process required a couple of design decisions. Each model has four layers, which start by increasing the dimensions of the data before shrinking them. This was just what I found to work best with the data I gathered. Secondly, the models are trained for 400 epochs; this choice was made because I started dramatically overfitting my data at 1000 epochs. Finally, here is the code to test the models:",
      "metadata": {}
    },
    {
      "id": "76d1de2a-7087-4d72-ad55-fda20c678d44",
      "cell_type": "code",
      "source": "def test_model(model, test_data, test_labels, threshold=0.4):\n    # Convert test data to PyTorch tensor\n    X_test = torch.tensor(test_data, dtype=torch.float32)\n    y_test = torch.tensor(test_labels, dtype=torch.float32).view(-1,1)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Pass test data through the model\n    with torch.no_grad():\n        outputs = model(X_test)\n\n    # Calculate loss (optional, depending on your requirements)\n    criterion = nn.MSELoss()\n    test_loss = criterion(outputs, y_test).item()\n    print(f'Test Loss: {test_loss:.4f}')\n\n    # Evaluate performance\n    num_correct = 0\n    for pred, true_label in zip(outputs, y_test):\n        print('predicted:', pred, 'label:', true_label)\n        if torch.abs(pred - true_label) <= threshold:\n            num_correct += 1\n\n    accuracy = num_correct / len(test_labels) * 100\n    print(f'Percentage of Correctly Labeled Test Data: {accuracy:.2f}%')\n\n    return accuracy",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6b32810f-d46b-40cf-b8af-8881496af097",
      "cell_type": "markdown",
      "source": "Now that the models have been fully created, there is an obvious question remaining: do the improve on the linear regression models?",
      "metadata": {}
    },
    {
      "id": "24bac31c-e93f-4728-8809-bc51d73cd1a6",
      "cell_type": "markdown",
      "source": "CONCLUSIONS:\n    Belvedere| Training Loss: 3.76 degrees Test Loss: 6.62 degrees\n    Midway| Training Loss: 4.49 degrees Test Loss: 7.92 degrees\n    Bergstrom| Training Loss: 2.19 degrees Test Loss: 3.34 degrees\n    Miami| Training Loss: 3.76 degrees Test Loss: 6.37 degrees",
      "metadata": {}
    },
    {
      "id": "835c17d6-1c24-4838-beb9-f69ea0692c5a",
      "cell_type": "markdown",
      "source": "These conclusions are not as promising as expected. It is essential to highlight that outliers skew the test loss. However, the models are still not much more accurate than a linear regression model. However, this might not be true when applied in real world settings.",
      "metadata": {}
    },
    {
      "id": "9202ba53-33f3-4765-95dc-71d32cea9c04",
      "cell_type": "markdown",
      "source": "The final part of the task was to code automated predictions. The strategy I used to do this was to first forecast all of the data I needed to make a prediction for that day, then have the model predict the data and bet on the day.",
      "metadata": {}
    },
    {
      "id": "02d9947f-cf0f-4119-9121-f145f9502eba",
      "cell_type": "code",
      "source": "def get_forecast(latitude, longitude, name):\n    today = datetime.now()\n    # Create a Point object with the coordinates\n    location = Point(latitude, longitude)\n\n    # Fetch forecast data from OpenWeatherMap API\n    api_key = key\n    if name == 'Belvedere':\n        response = requests.get(f'http://pro.openweathermap.org/data/2.5/weather?q=New York City, New York&APPID={key}')\n    elif name == 'Midway':\n        response = requests.get(f'http://pro.openweathermap.org/data/2.5/weather?q=Chicago, Illinois&APPID={key}')\n    elif name == 'Bergstrom':\n        response = requests.get(f'http://pro.openweathermap.org/data/2.5/weather?q=Austin, Texas&APPID={key}')\n    else:\n        response = requests.get(f'http://pro.openweathermap.org/data/2.5/weather?q=Miami, FLorida&APPID={key}')\n    openweather_data = response.json()\n    \n    meteo_data = Daily(location, today)\n    meteo_data = meteo_data.normalize()\n    meteo_data = meteo_data.fetch()\n    \n    #Temperature array\n    today = pd.Timestamp.now().normalize()\n    yesterday = today - pd.Timedelta(days=1)\n    max_temp_yesterday = Daily(location, yesterday).normalize().fetch()[meteo_data.index == yesterday.strftime('%Y-%m-%d')]['tmax'].values[0]\n    day_before_yesterday = yesterday - pd.Timedelta(days=1)\n    max_temp_day_before = Daily(location, day_before_yesterday).normalize().fetch()[meteo_data.index == day_before_yesterday.strftime('%Y-%m-%d')]['tmax'].values[0]\n    day_before_day_before_yesterday = day_before_yesterday - pd.Timedelta(days=1)\n    max_temp_day_before_day_before = Daily(location, day_before_day_before_yesterday).normalize().fetch()[meteo_data.index == day_before_day_before_yesterday.strftime('%Y-%m-%d')]['tmax'].values[0]\n    \n    prev_temp = [celsius_to_fahrenheit(max_temp_yesterday), celsius_to_fahrenheit(max_temp_day_before), celsius_to_fahrenheit(max_temp_day_before_day_before)]\n        \n    #PRCP\n    precipitation_rain = meteo_data[meteo_data.index == today.strftime('%Y-%m-%d')]['prcp'].fillna(0).values[0]\n    precipitation_snow = meteo_data[meteo_data.index == today.strftime('%Y-%m-%d')]['snow'].fillna(0).values[0]\n    precipitation = precipitation_snow + precipitation_rain\n    \n    #Humidity\n    humidity = openweather_data['main']['humidity']\n    \n    #sunlight\n    sunrise_timestamp = openweather_data['sys']['sunrise']\n    sunrise_datetime = datetime.fromtimestamp(sunrise_timestamp)\n    sunset_timestamp = openweather_data['sys']['sunset']\n    sunset_datetime = datetime.fromtimestamp(sunset_timestamp)\n    sunlight_duration_hours = datetime_to_hours_float(sunset_datetime - sunrise_datetime)\n    \n    #Wdir\n    wind_direction_degrees = meteo_data[meteo_data.index == today.strftime('%Y-%m-%d')]['wdir'].fillna(0).values[0]\n    \n    #WPGT\n    wind_speed = meteo_data[meteo_data.index == today.strftime('%Y-%m-%d')]['wspd'].fillna(0).values[0]\n    \n    #Pres\n    pressure = meteo_data[meteo_data.index == today.strftime('%Y-%m-%d')]['pres'].fillna(0).values[0]\n    \n    #co2 in air \n    # the datetime objects here are midnight yesterday and midnight today\n    co2_average = None\n    co2_change = None\n    if name != 'Bergstrom':\n        air_quality_history = get_co_in_air(latitude, longitude, datetime.combine(yesterday, datetime.min.time()), datetime.combine(today, datetime.min.time()))\n        co2_levels = [entry for entry in air_quality_history]\n        co2_values = [co2_levels[0], co2_levels[8], co2_levels[16]]\n        co2_average = sum(co2_values)/3\n        co2_change = co2_values[2] - co2_values[0]\n    \n    # Print forecast data\n    #print(\"Forecast for today:\")\n    #print(f\"Previous Temperatures: {prev_temp}\")\n    #print(f\"Precipitation: {precipitation} mm\")\n    #print(f\"Wind Direction: {wind_direction_degrees}\")\n    #print(f\"Wind Peak Gust: {wind_speed} m/s\")\n    #print(f\"Pressure: {pressure}\")\n    #print(f\"Humidity: {humidity}\")\n    #print(f\"Sunlight Duration: {sunlight_duration_hours} hours\")\n    #print(f\"Average CO2: {co2_average} ppm\")\n    #print(f\"Change in CO2: {co2_change}\")\n    #Previous Temps, prcp, wdir, wpgt, pres, prcp, sunlight, humidity, co2 average, co2 slope\n    if name != 'Bergstrom':\n        return np.array([prev_temp[0], prev_temp[1], prev_temp[2], precipitation, wind_direction_degrees, wind_speed, pressure, precipitation, sunlight_duration_hours, humidity, co2_average, co2_change ])\n    else:\n        return np.array([prev_temp[0], prev_temp[1], prev_temp[2], precipitation, wind_direction_degrees, wind_speed, pressure, precipitation, sunlight_duration_hours, humidity])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3c9f786d-9e92-4000-8603-6766dfaa12a0",
      "cell_type": "markdown",
      "source": "Next we use our trained models to predict the temperature.",
      "metadata": {}
    },
    {
      "id": "bc8d76a9-9949-4d37-a63b-2e60c8df4ec8",
      "cell_type": "code",
      "source": "def generate_prediction( latitude, longitude, name, model):\n    forecast = get_forecast(latitude, longitude,name)\n    return model.predict(forecast)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "35654a84-3842-4c1a-9762-2b6c3a759981",
      "cell_type": "markdown",
      "source": "Finally we have the computer place automatic bets.",
      "metadata": {}
    },
    {
      "id": "13359051-f6fc-44b5-8f85-db6dcd5f4634",
      "cell_type": "code",
      "source": "def bet_on_prediction(name, prediction):\n    config = kalshi_python.Configuration()\n    config.host = 'https://demo-api.kalshi.co/trade-api/v2'\n    kalshi_api = kalshi_python.ApiInstance(\n        email= email,\n        password= password,\n        configuration=config,\n    )\n    market_ticker = ''\n    side = ''\n    if name == 'Midway':\n        market_ticker = 'HIGHCHI-24APR02-T38'\n        if prediction < 38:\n            side = 'yes'\n        else:\n            side = 'no'\n    elif name == 'Bergstrom':\n        market_ticker = 'HIGHAUS-24APR02-T75'\n        if prediction > 75:\n            side = 'no'\n        else:\n            side = 'yes'\n    elif name == 'Belvedere':\n        market_ticker = 'HIGHNY-24APR02-B47.5'\n        if prediction < 48 and prediction > 47:\n            side = 'yes'\n        else:\n            side = 'no'\n    else:\n        market_ticker = 'HIGHMIA-24APR02-T79'\n        if prediction > 79:\n            side = 'no'\n        else:\n            side = 'yes'\n    exchange_status = kalshi_api.get_exchange_status()\n    print('Exchange status response: ')\n    print(exchange_status)\n    if exchange_status.trading_active:\n        # Submit an order for 10 yes contracts at 50cents on 'FED-23DEC-T3.00'.\n        order_uuid = str(uuid.uuid4())\n        order_response = kalshi_api.create_order(CreateOrderRequest(\n            ticker=market_ticker,\n            action='buy',\n            type='limit',\n            yes_price=50,\n            count=500,\n            client_order_id=order_uuid,\n            side=side,\n        ))\n        print('\\nOrder submitted: ')\n        pprint(order_response)\n    else:\n        print('\\nThe exchange is not trading active, no orders will be sent right now.')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f6efd4c3-4c13-4d84-98a0-3f4f04005e43",
      "cell_type": "markdown",
      "source": "EFFECTIVENESS:\nI started the model with 5000 dollars and ran it for five days. Over that period the change in networth was:\n    1: -132\n    2: 11\n    3: 269\n    4: -240 \n    5: 1\nFor a total change of -98. This was not a superb showing for the model, but with my limited testing I did discover that a couple of the features (precipitation and wind direction) were causing wild skews in the data. This implies that fundemental idea behind the model was sound, but the execution of the idea was flawed. ",
      "metadata": {}
    }
  ]
}